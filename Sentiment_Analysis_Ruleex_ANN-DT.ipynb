{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Ruleex ANN-DT for sentiment analysis for English Language. The dataset used is Large Movie Review Dataset (Imdb reviews) which have total of 50K reviews having equal distribution in training and test dataset (25K each). Furthermore, the distribution of positive and negative reviews in training and test dataset is also equal (12.5K pos and 12.5 neg reviews in each training and test dataset). Here we take the reduced data for demonstration since the alogorithm is very slow. The model in use is simple deep neural network with tf-idf featurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset location: https://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN-DT Paper: https://ieeexplore.ieee.org/document/809084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code reference: https://github.com/fantamat/ruleex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, use forked library: https://github.com/rohancode/ruleex_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gtrain import FCNet\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path of the directory where repository (https://github.com/rohancode/ruleex_modified) is cloned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/Rohan/Desktop/Cl/XAI/Rulex/workspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruleex_modified.ruleex.deepred.model import DeepRedFCNet\n",
    "import ruleex_modified.ruleex.anndt as ndt\n",
    "import ruleex_modified.ruleex.deepred as dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [01:41<00:00, 122.83it/s]\n",
      "100%|██████████| 12500/12500 [01:07<00:00, 184.43it/s]\n",
      "100%|██████████| 12500/12500 [03:40<00:00, 56.66it/s] \n",
      "100%|██████████| 12500/12500 [01:19<00:00, 157.95it/s]\n"
     ]
    }
   ],
   "source": [
    "def parse_folder(name):\n",
    "    data = []\n",
    "    for verdict in ('neg', 'pos'):\n",
    "        for file in tqdm(glob(os.path.join(name, verdict, '*.txt'))):\n",
    "            data.append({\n",
    "                'text': open(file, encoding='utf8').read(),\n",
    "                'verdict': verdict == 'pos'\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_train = parse_folder('/Users/Rohan/Desktop/Cl/XAI/aclImdb/train/')\n",
    "df_test = parse_folder('/Users/Rohan/Desktop/Cl/XAI/aclImdb/test/')\n",
    "\n",
    "df = pd.concat([df_train, df_test])\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(['index'], axis=1, inplace=True)\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     25000\n",
       "False    25000\n",
       "Name: verdict, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.verdict.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working on reduced data for to reduce runtime for quick demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     10082\n",
       "False     9918\n",
       "Name: verdict, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.verdict.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    sentence = remove_tags(sen)\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "X_text = []\n",
    "sentences = list(df['text'])\n",
    "for sen in sentences:\n",
    "    X_text.append(preprocess_text(sen))\n",
    "    \n",
    "y = df['verdict']\n",
    "y = np.array(list(map(lambda x: [0,1] if x==True else [1,0], y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text, x_text_test, y, y_test = train_test_split(X_text, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = vectorizer.fit_transform(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = vectorizer.transform(x_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Rohan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model_mn = Sequential([\n",
    "  Dense(100, activation='relu', input_shape=(500,)),\n",
    "  Dense(30, activation='relu'),\n",
    "  Dense(2, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mn.compile(\n",
    "  optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Rohan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/3\n",
      "16000/16000 [==============================] - 2s 134us/step - loss: 0.4291 - acc: 0.7983\n",
      "Epoch 2/3\n",
      "16000/16000 [==============================] - 2s 113us/step - loss: 0.3525 - acc: 0.8468\n",
      "Epoch 3/3\n",
      "16000/16000 [==============================] - 2s 103us/step - loss: 0.3351 - acc: 0.8504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12ccdb810>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mn.fit(\n",
    "  X_tfidf,\n",
    "  y,\n",
    "  epochs=3,\n",
    "  batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 77us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.37075235271453855, 0.83375]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mn.evaluate(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ruleex ANN-DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'sentiment_new123'\n",
    "data = 'imdb'\n",
    "method = 'anndt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = data + \"_\" + method\n",
    "tat_params = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tat_params[\"save_dir\"] = os.path.join(\"runs\", directory, name)\n",
    "if not os.path.isdir(tat_params[\"save_dir\"]):\n",
    "    os.makedirs(tat_params[\"save_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tat_params[\"init_restrictions\"] = np.array([[0, 1] for _ in range(500)])\n",
    "tat_params[\"act_val_num\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TaT_anndt(tr_in_dict, tr_out_dict, tst_in_dict, tst_out_dict, params):\n",
    "    \n",
    "    layer_sizes = []\n",
    "    layer_sizes.append(500)\n",
    "    for layer in model_mn.layers:\n",
    "        layer_sizes.append(layer.output_shape[-1])\n",
    "    \n",
    "    weights = model_mn.get_weights()\n",
    "    \n",
    "    net = DeepRedFCNet(layer_sizes)\n",
    "    net.init_eval_weights(weights)\n",
    "    \n",
    "    p = {\n",
    "        \"varbose\": 1,\n",
    "        ndt.ATTRIBUTE_SELECTION: False,\n",
    "    }\n",
    "\n",
    "    if \"min_train_samples\" in params:\n",
    "        p[ndt.MIN_TRAIN_SAMPLES] = params[\"min_train_samples\"]\n",
    "\n",
    "    if \"max_depth\" not in params:\n",
    "        p[ndt.MAX_DEPTH] = 10\n",
    "    else:\n",
    "        p[ndt.MAX_DEPTH] = params[\"max_depth\"]\n",
    "\n",
    "    if \"min_samples\" in params:\n",
    "        p[ndt.MIN_SAMPLES] = params[\"min_samples\"]\n",
    "\n",
    "    if \"min_split_fraction\" in params:\n",
    "        p[ndt.MIN_SPLIT_FRACTION] = params[\"min_split_fraction\"]\n",
    "\n",
    "    stat_test = None\n",
    "    if \"split_test\" in params:\n",
    "        if params[\"split_test\"]==\"t\":\n",
    "            stat_test = ndt.test_t\n",
    "        elif params[\"split_test\"] == \"welch\":\n",
    "            stat_test = ndt.test_welch\n",
    "        elif params[\"split_test\"] == \"chi2\":\n",
    "            stat_test = ndt.test_chi2\n",
    "        elif params[\"split_test\"] == \"f\":\n",
    "            stat_test = ndt.test_F\n",
    "        else:\n",
    "            stat_test = ndt.test_chi2\n",
    "    else:\n",
    "        stat_test = ndt.test_chi2\n",
    "\n",
    "    if \"measure\" in params:\n",
    "        if params[\"measure\"]==\"gini\":\n",
    "            measure = ndt.GiniMeasure\n",
    "        elif params[\"measure\"]==\"missclass\":\n",
    "            measure = ndt.FidelityGain\n",
    "        elif params[\"measure\"]==\"maxdiff\":\n",
    "            measure = ndt.MaxDifference\n",
    "        elif params[\"measure\"]==\"var\":\n",
    "            measure = ndt.VarianceMeasure\n",
    "        else:\n",
    "            measure = ndt.EntropyMeasure\n",
    "    else:\n",
    "        measure = ndt.EntropyMeasure\n",
    "\n",
    "    if \"attr_selection\" in params:\n",
    "        if params[\"attr_selection\"]==\"absvar\":\n",
    "            p[ndt.ATTRIBUTE_SELECTION] = ndt.MODE_ABSOLUTE_VARIATION\n",
    "        elif params[\"attr_selection\"]==\"missclass\":\n",
    "            p[ndt.ATTRIBUTE_SELECTION] = ndt.MODE_MISSCLASSIFICATION\n",
    "        elif params[\"attr_selection\"]==\"conmissclass\":\n",
    "            p[ndt.ATTRIBUTE_SELECTION] = ndt.MODE_CONTINUOUS_MISSCLASSIFICATION\n",
    "\n",
    "    if \"measure_weights\" in params:\n",
    "        if params[\"measure_weights\"]==\"train\":\n",
    "            p[ndt.MEASURE_WEIGHTS] = ndt.MODE_TRAIN\n",
    "        elif params[\"measure_weights\"]==\"all\":\n",
    "            p[ndt.MEASURE_WEIGHTS] = ndt.MODE_ALL\n",
    "        else:\n",
    "            p[ndt.MEASURE_WEIGHTS] = ndt.MODE_NONE\n",
    "\n",
    "    if \"force_sampling\" in params:\n",
    "        p[ndt.FORCE_SAMPLING] = params[\"force_sampling\"]\n",
    "\n",
    "    if \"num_samples\" in params:\n",
    "        indexes = np.random.permutation(len(tr_in_dict[\"x\"]))\n",
    "        x_train = tr_in_dict[\"x\"][indexes[:params[\"num_samples\"]]]\n",
    "    else:\n",
    "        x_train = tr_in_dict[\"x\"]\n",
    "\n",
    "    if \"vs_others\" in params:\n",
    "        model = lambda x: net.eval_binary_class(x, params[\"vs_others\"])\n",
    "    else:\n",
    "        model = net.eval\n",
    "\n",
    "    if \"init_restrictions\" in params:\n",
    "        res = params[\"init_restrictions\"]\n",
    "    else:\n",
    "        res = None\n",
    "\n",
    "    p[ndt.SPLIT_TEST_AFTER] = 3\n",
    "    rt = ndt.anndt(model, x_train, p,\n",
    "                   stat_test=stat_test,\n",
    "                   MeasureClass=measure,\n",
    "                   init_restrictions=res,\n",
    "                   sampler=ndt.BerNormalSampler(x_train, always_positive=True, sigma=0.01))\n",
    "    rt.save(os.path.join(params[\"save_dir\"], \"rt.pic\"))\n",
    "\n",
    "    dt = DecisionTreeClassifier(max_depth=p[\"max_depth\"])\n",
    "    dt.fit(x_train, np.argmax(model(x_train), axis=1))\n",
    "    dt = dr.sklearndt_to_ruletree(dt, one_class_on_leafs=True)\n",
    "    print(\"rt.view_graph()\")\n",
    "    rt.view_graph(filename='mnist_tree.pdf', varbose=True)\n",
    "    dt.save(os.path.join(params[\"save_dir\"], \"sklear_dt.pic\"))\n",
    "    inf = p[ndt.INF]\n",
    "    rt_train = rt.eval_all(tr_in_dict[\"x\"])\n",
    "    dt_train = dt.eval_all(tr_in_dict[\"x\"])\n",
    "    ln = np.argmax(model(tr_in_dict[\"x\"]), axis=1)\n",
    "    inf[\"fidelity\"] = np.mean(rt_train == ln)\n",
    "    inf[\"dt_fidelity\"] = np.mean(dt_train == ln)\n",
    "    rt_val = rt.eval_all(tst_in_dict[\"x\"])\n",
    "    dt_val = dt.eval_all(tst_in_dict[\"x\"])\n",
    "    l = np.argmax(tst_out_dict[\"y\"], axis=1)\n",
    "    ln = np.argmax(model(tst_in_dict[\"x\"]), axis=1)\n",
    "    inf[\"val_fidelity\"] = np.mean(ln == rt_val)\n",
    "    inf[\"val_accuracy\"] = np.mean(l == rt_val)\n",
    "    inf[\"dt_val_fidelity\"] = np.mean(ln == dt_val)\n",
    "    inf[\"dt_val_accuracy\"] = np.mean(l == dt_val)\n",
    "    print(\"Validation {}: fidelity {}, val_fidelity {}, val_accuracy {}\".format(params[\"act_val_num\"], inf[\"fidelity\"],\n",
    "                                                                                inf[\"val_fidelity\"],\n",
    "                                                                                inf[\"val_accuracy\"]))\n",
    "    print(\"Validation {}: fidelity {}, val_fidelity {}, val_accuracy {}\".format(params[\"act_val_num\"], inf[\"dt_fidelity\"],\n",
    "                                                                                inf[\"dt_val_fidelity\"],\n",
    "                                                                                inf[\"dt_val_accuracy\"]))\n",
    "    return inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_final = X_tfidf.toarray()\n",
    "x_test_final = X_test_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[anndt]: Generated new node with split x_37 > 0.019980312750856734 in train samples separation (3684, 12316)\n",
      "[anndt]: Generated new node with split x_173 > 0.037833732358758004 in train samples separation (713, 2971)\n",
      "[anndt]: Generated new node with split x_487 > 0.04590544015242135 in train samples separation (71, 642)\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_486 > 0.0645799244727236 in train samples separation (35, 607)\n",
      "[anndt]: Generating 15 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_399 > 0.014609228736498628 in train samples separation (36, 571)\n",
      "[anndt]: Generating 14 new samples.\n",
      "[anndt]: Generated new node with split x_13 > 0.15866379503947015 in train samples separation (1, 35)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 2 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_311 > 0.03378097173536705 in train samples separation (39, 532)\n",
      "[anndt]: Generating 11 new samples.\n",
      "[anndt]: Generated new node with split x_78 > 0.06311882454422041 in train samples separation (2, 37)\n",
      "[anndt]: Generating 48 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 2 new samples.\n",
      "[anndt]: Generated new node with split x_68 > 0.04951919570468773 in train samples separation (3, 34)\n",
      "[anndt]: Generating 47 new samples.\n",
      "[anndt]: Generated new node with split x_13 > 0.02725537653460436 in train samples separation (2, 1)\n",
      "[anndt]: Generating 19 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 3 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_130 > 0.050464277911372354 in train samples separation (49, 483)\n",
      "[anndt]: Generating 1 new samples.\n",
      "[anndt]: Generated new node with split x_252 > 0.08813442480512652 in train samples separation (5, 44)\n",
      "[anndt]: Generating 44 new samples.\n",
      "[anndt]: Generated new node with split x_5 > 0.06159803964659177 in train samples separation (1, 4)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 7 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 6 new samples.\n",
      "[anndt]: Generated new node with split x_207 > 0.05710635692570576 in train samples separation (32, 12)\n",
      "[anndt]: Generating 14 new samples.\n",
      "[anndt]: Generated new node with split x_52 > 0.07059210306166963 in train samples separation (0, 32)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generating 36 new samples.\n",
      "[anndt]: Generated new node with split x_173 > 0.08359498041196976 in train samples separation (7, 5)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_252 > 0.04241635672062479 in train samples separation (110, 373)\n",
      "[anndt]: Generated new node with split x_79 > 0.09057241085235747 in train samples separation (5, 105)\n",
      "[anndt]: Generating 45 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_57 > 0.08587149438828204 in train samples separation (3, 102)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_44 > 0.07451119655249361 in train samples separation (34, 339)\n",
      "[anndt]: Generating 16 new samples.\n",
      "[anndt]: Generated new node with split x_496 > 0.10056579291288073 in train samples separation (2, 32)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_35 > 0.05324150302779881 in train samples separation (16, 323)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_487 > 0.007221454764675958 in train samples separation (562, 2409)\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_37 > 0.10706967291237585 in train samples separation (718, 1691)\n",
      "[anndt]: Generated new node with split x_4 > 0.011089437131085362 in train samples separation (298, 420)\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_171 > 0.13368499006708584 in train samples separation (39, 381)\n",
      "[anndt]: Generating 11 new samples.\n",
      "[anndt]: Generated new node with split x_62 > 0.08023414510144752 in train samples separation (5, 34)\n",
      "[anndt]: Generating 43 new samples.\n",
      "[anndt]: Generated new node with split x_298 > 0.11685405548455324 in train samples separation (0, 5)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 3 new samples.\n",
      "[anndt]: Generated new node with split x_37 > 0.13175449454911678 in train samples separation (4, 1)\n",
      "[anndt]: Generating 17 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 7 new samples.\n",
      "[anndt]: Generated new node with split x_218 > 0.010456845550430829 in train samples separation (12, 22)\n",
      "[anndt]: Generating 34 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 16 new samples.\n",
      "[anndt]: Generated new node with split x_273 > 0.0456308187856119 in train samples separation (5, 17)\n",
      "[anndt]: Generating 39 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 11 new samples.\n",
      "[anndt]: Generated new node with split x_24 > 0.15818947751679724 in train samples separation (6, 11)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_497 > 0.07517127684033217 in train samples separation (130, 251)\n",
      "[anndt]: Generated new node with split x_448 > 0.11403374827214638 in train samples separation (11, 119)\n",
      "[anndt]: Generating 39 new samples.\n",
      "[anndt]: Generated new node with split x_455 > 0.06501316485517195 in train samples separation (5, 6)\n",
      "[anndt]: Generating 26 new samples.\n",
      "[anndt]: Generated new node with split x_5 > 0.08660395409461968 in train samples separation (1, 4)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generating 24 new samples.\n",
      "[anndt]: Generated new node with split x_237 > 0.11942850587011572 in train samples separation (0, 6)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_489 > 0.1078972471687264 in train samples separation (6, 113)\n",
      "[anndt]: Generating 44 new samples.\n",
      "[anndt]: Generated new node with split x_289 > 0.0919074428309552 in train samples separation (1, 5)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_462 > 0.09936893679208451 in train samples separation (9, 104)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_130 > 0.07365613637828422 in train samples separation (8, 243)\n",
      "[anndt]: Generating 42 new samples.\n",
      "[anndt]: Generated new node with split x_181 > 0.035917663272427236 in train samples separation (2, 6)\n",
      "[anndt]: Generating 38 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 12 new samples.\n",
      "[anndt]: Generated new node with split x_26 > 0.10993354790122106 in train samples separation (1, 5)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_313 > 0.09795333123993587 in train samples separation (5, 238)\n",
      "[anndt]: Generating 45 new samples.\n",
      "[anndt]: Generated new node with split x_24 > 0.1814878033360091 in train samples separation (2, 3)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[anndt]: Generated new node with split x_40 > 0.06283169766609716 in train samples separation (9, 229)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_35 > 0.03201852170373348 in train samples separation (154, 1537)\n",
      "[anndt]: Generated new node with split x_203 > 0.10019829254180088 in train samples separation (5, 149)\n",
      "[anndt]: Generating 45 new samples.\n",
      "[anndt]: Generated new node with split x_13 > 0.09510870613746103 in train samples separation (2, 3)\n",
      "[anndt]: Generating 35 new samples.\n",
      "[anndt]: Generated new node with split x_37 > 0.0659649499786345 in train samples separation (0, 2)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 14 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 15 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_487 > 0.02558263301098437 in train samples separation (767, 11549)\n",
      "[anndt]: Generated new node with split x_173 > 0.022592118572499584 in train samples separation (121, 646)\n",
      "[anndt]: Generated new node with split x_237 > 0.03873720284506077 in train samples separation (21, 100)\n",
      "[anndt]: Generating 29 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_190 > 0.012868526333789404 in train samples separation (23, 77)\n",
      "[anndt]: Generating 27 new samples.\n",
      "[anndt]: Generated new node with split x_272 > 0.029676809168910502 in train samples separation (8, 15)\n",
      "[anndt]: Generating 31 new samples.\n",
      "[anndt]: Generated new node with split x_32 > 0.07805546213614079 in train samples separation (2, 6)\n",
      "[anndt]: Generating 48 new samples.\n",
      "[anndt]: Generated new node with split x_481 > 0.044470111048573464 in train samples separation (0, 2)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 13 new samples.\n",
      "[anndt]: Generated new node with split x_73 > 0.029642023003163487 in train samples separation (0, 2)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 5 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 2 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 19 new samples.\n",
      "[anndt]: Generated new node with split x_102 > 0.021493141818400306 in train samples separation (2, 13)\n",
      "[anndt]: Generating 44 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 6 new samples.\n",
      "[anndt]: Generated new node with split x_369 > 0.04242841147787123 in train samples separation (1, 12)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 3 new samples.\n",
      "[anndt]: Generated new node with split x_169 > 0.07892251480172482 in train samples separation (0, 12)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 6 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_320 > 0.08268885777113942 in train samples separation (4, 73)\n",
      "[anndt]: Generating 46 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_225 > 0.07971891323777418 in train samples separation (3, 70)\n",
      "[anndt]: Generating 47 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_463 > 0.03839443500685229 in train samples separation (21, 49)\n",
      "[anndt]: Generating 29 new samples.\n",
      "[anndt]: Generated new node with split x_13 > 0.041098261426796734 in train samples separation (14, 7)\n",
      "[anndt]: Generating 17 new samples.\n",
      "[anndt]: Generated new node with split x_300 > 0.054035437805641615 in train samples separation (4, 10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generating 33 new samples.\n",
      "[anndt]: Generated new node with split x_99 > 0.02851369205114959 in train samples separation (2, 5)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generating 1 new samples.\n",
      "[anndt]: Generated new node with split x_26 > 0.12413495233200707 in train samples separation (1, 48)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 1 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_412 > 0.12397829432683677 in train samples separation (216, 430)\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_173 > 0.0572562159798813 in train samples separation (2264, 9285)\n",
      "[anndt]: Generated new node with split x_173 > 0.11970192358700679 in train samples separation (746, 1518)\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_486 > 0.0365694218712396 in train samples separation (35, 1483)\n",
      "[anndt]: Generating 15 new samples.\n",
      "[anndt]: Generated new node with split x_307 > 0.07252185716086579 in train samples separation (5, 30)\n",
      "[anndt]: Generating 42 new samples.\n",
      "[anndt]: Generated new node with split x_17 > 0.022234166158197365 in train samples separation (3, 2)\n",
      "[anndt]: Generating 18 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 32 new samples.\n",
      "[anndt]: Generated new node with split x_6 > 0.09420436023545405 in train samples separation (0, 2)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 9 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 8 new samples.\n",
      "[anndt]: Generated new node with split x_49 > 0.13993920708198693 in train samples separation (1, 29)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 1 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_457 > 0.03611266379697614 in train samples separation (370, 8915)\n",
      "[anndt]: Generated new node with split x_311 > 0.048465198887178075 in train samples separation (10, 360)\n",
      "[anndt]: Generating 40 new samples.\n",
      "[anndt]: Generated new node with split x_32 > 0.005341988465131751 in train samples separation (6, 4)\n",
      "[anndt]: Generating 20 new samples.\n",
      "[anndt]: Generated new node with split x_29 > 0.09636032483756196 in train samples separation (2, 4)\n",
      "[anndt]: Generating 40 new samples.\n",
      "[anndt]: Generated new node with split x_16 > 0.060792162686133466 in train samples separation (0, 2)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 5 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 10 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 30 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_417 > 0.05454687346529814 in train samples separation (47, 313)\n",
      "[anndt]: Generating 3 new samples.\n",
      "[anndt]: Generated new node with split x_454 > 0.026454606307275662 in train samples separation (3, 44)\n",
      "[anndt]: Generating 47 new samples.\n",
      "[anndt]: Generated new node with split x_47 > 0.1090358978475964 in train samples separation (0, 3)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 6 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 3 new samples.\n",
      "[anndt]: Generated new node with split x_173 > 0.04620444222200155 in train samples separation (2, 42)\n",
      "[anndt]: Generating 48 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 2 new samples.\n",
      "[anndt]: Generated new node with split x_445 > 0.06029376097997957 in train samples separation (3, 39)\n",
      "[anndt]: Generating 45 new samples.\n",
      "[anndt]: Generated new node with split x_362 > 0.22180359114505344 in train samples separation (1, 2)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generating 5 new samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_35 > 0.040271652091013285 in train samples separation (319, 8596)\n",
      "[anndt]: Generated new node with split x_24 > 0.3051917245982442 in train samples separation (8, 311)\n",
      "[anndt]: Generating 42 new samples.\n",
      "[anndt]: Generated new node with split x_422 > 0.11995254548475696 in train samples separation (5, 3)\n",
      "[anndt]: Generating 21 new samples.\n",
      "[anndt]: Generated new node with split x_396 > 0.09224531488736235 in train samples separation (1, 4)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 16 new samples.\n",
      "[anndt]: Generated new node with split x_7 > 0.08979370922456203 in train samples separation (0, 4)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 4 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 29 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_325 > 0.052825186688217514 in train samples separation (428, 8168)\n",
      "[anndt]: Generated new node with split x_298 > 0.03167607469034199 in train samples separation (149, 279)\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_290 > 0.138908764258931 in train samples separation (144, 135)\n",
      "[anndt]: Generated new node with split x_406 > 0.06121793209253165 in train samples separation (29, 115)\n",
      "[anndt]: Generating 21 new samples.\n",
      "[anndt]: Generated new node with split x_17 > 0.047539109883447034 in train samples separation (2, 27)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_301 > 0.07468724321409247 in train samples separation (11, 104)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_61 > 0.043418649444265434 in train samples separation (67, 68)\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_412 > 0.05971486413892593 in train samples separation (45, 23)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_130 > 0.0488077932463612 in train samples separation (576, 7592)\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_288 > 0.03975980873105582 in train samples separation (799, 6793)\n",
      "[anndt]: Generated new node with split x_482 > 0.014815011177879346 in train samples separation (42, 757)\n",
      "[anndt]: Generating 8 new samples.\n",
      "[anndt]: Generated new node with split x_150 > 0.009470837033207764 in train samples separation (31, 11)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_108 > 0.08505428677859618 in train samples separation (27, 730)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_56 > 0.045498435462817725 in train samples separation (279, 6514)\n",
      "[anndt]: Generated new node with split x_40 > 0.09256133719958623 in train samples separation (6, 273)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_286 > 0.04758942686684972 in train samples separation (1371, 5143)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "rt.view_graph()\n",
      "Validation 0: fidelity 0.7643125, val_fidelity 0.74125, val_accuracy 0.7085\n",
      "Validation 0: fidelity 0.7809375, val_fidelity 0.74725, val_accuracy 0.717\n"
     ]
    }
   ],
   "source": [
    "inf = TaT_anndt({\"x\": x_train_final}, {\"y\": y}, {\"x\": x_test_final}, {\"y\": y_test}, tat_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Visualization (.pdf) highlighting extracted rules will be saved in the current directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
