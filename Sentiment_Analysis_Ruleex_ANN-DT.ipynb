{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Ruleex ANN-DT for sentiment analysis for English Language. The dataset used is Large Movie Review Dataset (Imdb reviews) which have total of 50K reviews having equal distribution in training and test dataset (25K each). Furthermore, the distribution of positive and negative reviews in training and test dataset is also equal (12.5K pos and 12.5 neg reviews in each training and test dataset). Here we take the reduced data for demonstration since the alogorithm is very slow. The model in use is simple deep neural network with tf-idf featurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset location: https://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN-DT Paper: https://ieeexplore.ieee.org/document/809084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code reference: https://github.com/fantamat/ruleex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, use forked library: https://github.com/rohancode/ruleex_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gtrain import FCNet\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path of the directory where repository (https://github.com/rohancode/ruleex_modified) is cloned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/Users/Rohan/Desktop/Cl/XAI/Rulex/workspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruleex_modified.ruleex.deepred.model import DeepRedFCNet\n",
    "import ruleex_modified.ruleex.anndt as ndt\n",
    "import ruleex_modified.ruleex.deepred as dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:00<00:00, 14004.46it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 13699.22it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 13956.11it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 12917.82it/s]\n"
     ]
    }
   ],
   "source": [
    "def parse_folder(name):\n",
    "    data = []\n",
    "    for verdict in ('neg', 'pos'):\n",
    "        for file in tqdm(glob(os.path.join(name, verdict, '*.txt'))):\n",
    "            data.append({\n",
    "                'text': open(file, encoding='utf8').read(),\n",
    "                'verdict': verdict == 'pos'\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df_train = parse_folder('/Users/Rohan/Desktop/Cl/XAI/aclImdb/train/')\n",
    "df_test = parse_folder('/Users/Rohan/Desktop/Cl/XAI/aclImdb/test/')\n",
    "\n",
    "df = pd.concat([df_train, df_test])\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(['index'], axis=1, inplace=True)\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     25000\n",
       "False    25000\n",
       "Name: verdict, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.verdict.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working on reduced data for to reduce runtime for quick demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    10026\n",
       "True      9974\n",
       "Name: verdict, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.verdict.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    sentence = remove_tags(sen)\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "X_text = []\n",
    "sentences = list(df['text'])\n",
    "for sen in sentences:\n",
    "    X_text.append(preprocess_text(sen))\n",
    "    \n",
    "y = df['verdict']\n",
    "y = np.array(list(map(lambda x: [0,1] if x==True else [1,0], y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text, x_text_test, y, y_test = train_test_split(X_text, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf = vectorizer.fit_transform(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf = vectorizer.transform(x_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Rohan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model_mn = Sequential([\n",
    "  Dense(100, activation='relu', input_shape=(500,)),\n",
    "  Dense(30, activation='relu'),\n",
    "  Dense(2, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mn.compile(\n",
    "  optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Rohan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/3\n",
      "16000/16000 [==============================] - 2s 126us/step - loss: 0.4319 - acc: 0.7970\n",
      "Epoch 2/3\n",
      "16000/16000 [==============================] - 2s 100us/step - loss: 0.3504 - acc: 0.8462\n",
      "Epoch 3/3\n",
      "16000/16000 [==============================] - 2s 94us/step - loss: 0.3367 - acc: 0.8546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12e72b510>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mn.fit(\n",
    "  X_tfidf,\n",
    "  y,\n",
    "  epochs=3,\n",
    "  batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 47us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3848508437871933, 0.8285]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mn.evaluate(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ruleex ANN-DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'sentiment_new123'\n",
    "data = 'imdb'\n",
    "method = 'anndt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = data + \"_\" + method\n",
    "tat_params = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tat_params[\"save_dir\"] = os.path.join(\"runs\", directory, name)\n",
    "if not os.path.isdir(tat_params[\"save_dir\"]):\n",
    "    os.makedirs(tat_params[\"save_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tat_params[\"init_restrictions\"] = np.array([[0, 1] for _ in range(500)])\n",
    "tat_params[\"act_val_num\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TaT_anndt(tr_in_dict, tr_out_dict, tst_in_dict, tst_out_dict, params):\n",
    "    \n",
    "    layer_sizes = []\n",
    "    layer_sizes.append(500)\n",
    "    for layer in model_mn.layers:\n",
    "        layer_sizes.append(layer.output_shape[-1])\n",
    "    \n",
    "    weights = model_mn.get_weights()\n",
    "    \n",
    "    net = DeepRedFCNet(layer_sizes)\n",
    "    net.init_eval_weights(weights)\n",
    "    \n",
    "    p = {\n",
    "        \"varbose\": 1,\n",
    "        ndt.ATTRIBUTE_SELECTION: False,\n",
    "    }\n",
    "\n",
    "    if \"min_train_samples\" in params:\n",
    "        p[ndt.MIN_TRAIN_SAMPLES] = params[\"min_train_samples\"]\n",
    "\n",
    "    if \"max_depth\" not in params:\n",
    "        p[ndt.MAX_DEPTH] = 10\n",
    "    else:\n",
    "        p[ndt.MAX_DEPTH] = params[\"max_depth\"]\n",
    "\n",
    "    if \"min_samples\" in params:\n",
    "        p[ndt.MIN_SAMPLES] = params[\"min_samples\"]\n",
    "\n",
    "    if \"min_split_fraction\" in params:\n",
    "        p[ndt.MIN_SPLIT_FRACTION] = params[\"min_split_fraction\"]\n",
    "\n",
    "    stat_test = None\n",
    "    if \"split_test\" in params:\n",
    "        if params[\"split_test\"]==\"t\":\n",
    "            stat_test = ndt.test_t\n",
    "        elif params[\"split_test\"] == \"welch\":\n",
    "            stat_test = ndt.test_welch\n",
    "        elif params[\"split_test\"] == \"chi2\":\n",
    "            stat_test = ndt.test_chi2\n",
    "        elif params[\"split_test\"] == \"f\":\n",
    "            stat_test = ndt.test_F\n",
    "        else:\n",
    "            stat_test = ndt.test_chi2\n",
    "    else:\n",
    "        stat_test = ndt.test_chi2\n",
    "\n",
    "    if \"measure\" in params:\n",
    "        if params[\"measure\"]==\"gini\":\n",
    "            measure = ndt.GiniMeasure\n",
    "        elif params[\"measure\"]==\"missclass\":\n",
    "            measure = ndt.FidelityGain\n",
    "        elif params[\"measure\"]==\"maxdiff\":\n",
    "            measure = ndt.MaxDifference\n",
    "        elif params[\"measure\"]==\"var\":\n",
    "            measure = ndt.VarianceMeasure\n",
    "        else:\n",
    "            measure = ndt.EntropyMeasure\n",
    "    else:\n",
    "        measure = ndt.EntropyMeasure\n",
    "\n",
    "    if \"attr_selection\" in params:\n",
    "        if params[\"attr_selection\"]==\"absvar\":\n",
    "            p[ndt.ATTRIBUTE_SELECTION] = ndt.MODE_ABSOLUTE_VARIATION\n",
    "        elif params[\"attr_selection\"]==\"missclass\":\n",
    "            p[ndt.ATTRIBUTE_SELECTION] = ndt.MODE_MISSCLASSIFICATION\n",
    "        elif params[\"attr_selection\"]==\"conmissclass\":\n",
    "            p[ndt.ATTRIBUTE_SELECTION] = ndt.MODE_CONTINUOUS_MISSCLASSIFICATION\n",
    "\n",
    "    if \"measure_weights\" in params:\n",
    "        if params[\"measure_weights\"]==\"train\":\n",
    "            p[ndt.MEASURE_WEIGHTS] = ndt.MODE_TRAIN\n",
    "        elif params[\"measure_weights\"]==\"all\":\n",
    "            p[ndt.MEASURE_WEIGHTS] = ndt.MODE_ALL\n",
    "        else:\n",
    "            p[ndt.MEASURE_WEIGHTS] = ndt.MODE_NONE\n",
    "\n",
    "    if \"force_sampling\" in params:\n",
    "        p[ndt.FORCE_SAMPLING] = params[\"force_sampling\"]\n",
    "\n",
    "    if \"num_samples\" in params:\n",
    "        indexes = np.random.permutation(len(tr_in_dict[\"x\"]))\n",
    "        x_train = tr_in_dict[\"x\"][indexes[:params[\"num_samples\"]]]\n",
    "    else:\n",
    "        x_train = tr_in_dict[\"x\"]\n",
    "\n",
    "    if \"vs_others\" in params:\n",
    "        model = lambda x: net.eval_binary_class(x, params[\"vs_others\"])\n",
    "    else:\n",
    "        model = net.eval\n",
    "\n",
    "    if \"init_restrictions\" in params:\n",
    "        res = params[\"init_restrictions\"]\n",
    "    else:\n",
    "        res = None\n",
    "\n",
    "    p[ndt.SPLIT_TEST_AFTER] = 3\n",
    "    rt = ndt.anndt(model, x_train, p,\n",
    "                   stat_test=stat_test,\n",
    "                   MeasureClass=measure,\n",
    "                   init_restrictions=res,\n",
    "                   sampler=ndt.BerNormalSampler(x_train, always_positive=True, sigma=0.01))\n",
    "    rt.save(os.path.join(params[\"save_dir\"], \"rt.pic\"))\n",
    "\n",
    "    dt = DecisionTreeClassifier(max_depth=p[\"max_depth\"])\n",
    "    dt.fit(x_train, np.argmax(model(x_train), axis=1))\n",
    "    dt = dr.sklearndt_to_ruletree(dt, one_class_on_leafs=True)\n",
    "    print(\"rt.view_graph()\")\n",
    "    rt.view_graph(filename='mnist_tree.pdf', varbose=True)\n",
    "    dt.save(os.path.join(params[\"save_dir\"], \"sklear_dt.pic\"))\n",
    "    inf = p[ndt.INF]\n",
    "    rt_train = rt.eval_all(tr_in_dict[\"x\"])\n",
    "    dt_train = dt.eval_all(tr_in_dict[\"x\"])\n",
    "    ln = np.argmax(model(tr_in_dict[\"x\"]), axis=1)\n",
    "    inf[\"fidelity\"] = np.mean(rt_train == ln)\n",
    "    inf[\"dt_fidelity\"] = np.mean(dt_train == ln)\n",
    "    rt_val = rt.eval_all(tst_in_dict[\"x\"])\n",
    "    dt_val = dt.eval_all(tst_in_dict[\"x\"])\n",
    "    l = np.argmax(tst_out_dict[\"y\"], axis=1)\n",
    "    ln = np.argmax(model(tst_in_dict[\"x\"]), axis=1)\n",
    "    inf[\"val_fidelity\"] = np.mean(ln == rt_val)\n",
    "    inf[\"val_accuracy\"] = np.mean(l == rt_val)\n",
    "    inf[\"dt_val_fidelity\"] = np.mean(ln == dt_val)\n",
    "    inf[\"dt_val_accuracy\"] = np.mean(l == dt_val)\n",
    "    print(\"Validation {}: fidelity {}, val_fidelity {}, val_accuracy {}\".format(params[\"act_val_num\"], inf[\"fidelity\"],\n",
    "                                                                                inf[\"val_fidelity\"],\n",
    "                                                                                inf[\"val_accuracy\"]))\n",
    "    print(\"Validation {}: fidelity {}, val_fidelity {}, val_accuracy {}\".format(params[\"act_val_num\"], inf[\"dt_fidelity\"],\n",
    "                                                                                inf[\"dt_val_fidelity\"],\n",
    "                                                                                inf[\"dt_val_accuracy\"]))\n",
    "    return inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_final = X_tfidf.toarray()\n",
    "x_test_final = X_test_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[anndt]: Generated new node with split x_37 > 0.019214667040972223 in train samples separation (3797, 12203)\n",
      "[anndt]: Generated new node with split x_175 > 0.052310866126188585 in train samples separation (561, 3236)\n",
      "[anndt]: Generated new node with split x_487 > 0.01358462706339282 in train samples separation (64, 497)\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_131 > 0.05004814777826065 in train samples separation (46, 451)\n",
      "[anndt]: Generating 4 new samples.\n",
      "[anndt]: Generated new node with split x_301 > 0.06701800203651462 in train samples separation (5, 41)\n",
      "[anndt]: Generating 44 new samples.\n",
      "[anndt]: Generated new node with split x_288 > 0.03739006353521823 in train samples separation (2, 3)\n",
      "[anndt]: Generating 25 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 25 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 6 new samples.\n",
      "[anndt]: Generated new node with split x_243 > 0.07931269791813354 in train samples separation (2, 39)\n",
      "[anndt]: Generating 48 new samples.\n",
      "[anndt]: Generated new node with split x_4 > 0.0652020016404844 in train samples separation (0, 2)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 8 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 2 new samples.\n",
      "[anndt]: Generated new node with split x_13 > 0.09130889170350819 in train samples separation (1, 38)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 1 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_49 > 0.09831933457824624 in train samples separation (28, 423)\n",
      "[anndt]: Generating 22 new samples.\n",
      "[anndt]: Generated new node with split x_135 > 0.08171820545065611 in train samples separation (4, 24)\n",
      "[anndt]: Generating 44 new samples.\n",
      "[anndt]: Generated new node with split x_18 > 0.10189636676805598 in train samples separation (1, 3)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 16 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 6 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_37 > 0.08978970658394828 in train samples separation (150, 273)\n",
      "[anndt]: Generated new node with split x_175 > 0.14342300197424004 in train samples separation (22, 128)\n",
      "[anndt]: Generating 28 new samples.\n",
      "[anndt]: Generated new node with split x_297 > 0.09101212582288398 in train samples separation (6, 16)\n",
      "[anndt]: Generating 33 new samples.\n",
      "[anndt]: Generated new node with split x_126 > 0.23230073941192608 in train samples separation (0, 6)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 9 new samples.\n",
      "[anndt]: Generated new node with split x_16 > 0.1081929515927594 in train samples separation (0, 6)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generating 17 new samples.\n",
      "[anndt]: Generated new node with split x_306 > 0.06629091808115492 in train samples separation (3, 13)\n",
      "[anndt]: Generating 35 new samples.\n",
      "[anndt]: Generated new node with split x_477 > 0.07600854801174194 in train samples separation (0, 3)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generating 15 new samples.\n",
      "[anndt]: Generated new node with split x_13 > 0.05980032403363422 in train samples separation (1, 12)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_122 > 0.12366712385209183 in train samples separation (6, 122)\n",
      "[anndt]: Generating 44 new samples.\n",
      "[anndt]: Generated new node with split x_5 > 0.07224020280194426 in train samples separation (1, 5)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 8 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_375 > 0.05590224377054426 in train samples separation (3, 119)\n",
      "[anndt]: Generating 47 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_219 > 0.04711100338994273 in train samples separation (109, 164)\n",
      "[anndt]: Generated new node with split x_115 > 0.10686067473272137 in train samples separation (5, 104)\n",
      "[anndt]: Generating 45 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_288 > 0.04164743406860357 in train samples separation (31, 73)\n",
      "[anndt]: Generating 19 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_117 > 0.04319562403803264 in train samples separation (5, 68)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_400 > 0.08552603860947507 in train samples separation (15, 149)\n",
      "[anndt]: Generating 35 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_449 > 0.07089554440649212 in train samples separation (31, 118)\n",
      "[anndt]: Generating 19 new samples.\n",
      "[anndt]: Generated new node with split x_147 > 0.027082870362098357 in train samples separation (3, 28)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_182 > 0.06048301847932851 in train samples separation (11, 107)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_487 > 0.04354438246556202 in train samples separation (502, 2734)\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_131 > 0.03479777093513772 in train samples separation (124, 2610)\n",
      "[anndt]: Generated new node with split x_235 > 0.02325039165667285 in train samples separation (65, 59)\n",
      "[anndt]: Generated new node with split x_456 > 0.05871259095592911 in train samples separation (41, 24)\n",
      "[anndt]: Generating 9 new samples.\n",
      "[anndt]: Generated new node with split x_468 > 0.09396875036982102 in train samples separation (3, 38)\n",
      "[anndt]: Generating 47 new samples.\n",
      "[anndt]: Generated new node with split x_1 > 0.10294479406365056 in train samples separation (0, 3)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 7 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 3 new samples.\n",
      "[anndt]: Generated new node with split x_366 > 0.05260453537251279 in train samples separation (5, 33)\n",
      "[anndt]: Generating 44 new samples.\n",
      "[anndt]: Generated new node with split x_347 > 0.05092551432315131 in train samples separation (2, 3)\n",
      "[anndt]: Generating 41 new samples.\n",
      "[anndt]: Generated new node with split x_347 > 0.05486782849071646 in train samples separation (0, 2)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generating 9 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 6 new samples.\n",
      "[anndt]: Generated new node with split x_3 > 0.04791773973514995 in train samples separation (0, 33)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 2 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 26 new samples.\n",
      "[anndt]: Generated new node with split x_23 > 0.21414450165379234 in train samples separation (13, 11)\n",
      "[anndt]: Generating 22 new samples.\n",
      "[anndt]: Generated new node with split x_125 > 0.04493105558625492 in train samples separation (1, 12)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 8 new samples.\n",
      "[anndt]: Generated new node with split x_0 > 0.05262661356026743 in train samples separation (0, 12)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 2 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 28 new samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[anndt]: Generated new node with split x_218 > 0.019301516921505422 in train samples separation (3, 8)\n",
      "[anndt]: Generating 43 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 7 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_473 > 0.049124185275493344 in train samples separation (18, 41)\n",
      "[anndt]: Generating 32 new samples.\n",
      "[anndt]: Generated new node with split x_15 > 0.08870415241524131 in train samples separation (0, 18)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 2 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 9 new samples.\n",
      "[anndt]: Generated new node with split x_462 > 0.04155629930465232 in train samples separation (8, 33)\n",
      "[anndt]: Generating 40 new samples.\n",
      "[anndt]: Generated new node with split x_7 > 0.20233480799291859 in train samples separation (1, 7)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 2 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 10 new samples.\n",
      "[anndt]: Generated new node with split x_474 > 0.040610245199255574 in train samples separation (3, 30)\n",
      "[anndt]: Generating 45 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 5 new samples.\n",
      "[anndt]: Generated new node with split x_475 > 0.022149346678957037 in train samples separation (3, 27)\n",
      "[anndt]: Generating 46 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 4 new samples.\n",
      "[anndt]: Generated new node with split x_94 > 0.022162884888705447 in train samples separation (2, 25)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_37 > 0.10492409982816212 in train samples separation (789, 1821)\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_458 > 0.03508687368770405 in train samples separation (141, 1680)\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_424 > 0.04589164861155341 in train samples separation (55, 1625)\n",
      "[anndt]: Generated new node with split x_283 > 0.09684150556202528 in train samples separation (7, 48)\n",
      "[anndt]: Generating 43 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 2 new samples.\n",
      "[anndt]: Generated new node with split x_246 > 0.06530298038987521 in train samples separation (6, 42)\n",
      "[anndt]: Generating 44 new samples.\n",
      "[anndt]: Generated new node with split x_93 > 0.09659599875873773 in train samples separation (0, 6)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generating 6 new samples.\n",
      "[anndt]: Generated new node with split x_327 > 0.04558431977858446 in train samples separation (4, 38)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_486 > 0.03675543845223286 in train samples separation (145, 1480)\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_124 > 0.06846149734851456 in train samples separation (246, 1234)\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_234 > 0.08943262548521261 in train samples separation (53, 1181)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_487 > 0.0252244143886104 in train samples separation (820, 11383)\n",
      "[anndt]: Generated new node with split x_487 > 0.11723964409909418 in train samples separation (354, 466)\n",
      "[anndt]: Generated new node with split x_309 > 0.11750056410803966 in train samples separation (9, 345)\n",
      "[anndt]: Generating 41 new samples.\n",
      "[anndt]: Generated new node with split x_23 > 0.17745881468157712 in train samples separation (2, 7)\n",
      "[anndt]: Generating 37 new samples.\n",
      "[anndt]: Generated new node with split x_487 > 0.15157353931851847 in train samples separation (0, 2)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 10 new samples.\n",
      "[anndt]: Generated new node with split x_99 > 0.10913127301033737 in train samples separation (0, 2)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 10 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 13 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_175 > 0.054204577195320854 in train samples separation (2323, 9060)\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_458 > 0.04741257890265903 in train samples separation (342, 8718)\n",
      "[anndt]: Generated new node with split x_203 > 0.043818224911554605 in train samples separation (17, 325)\n",
      "[anndt]: Generating 33 new samples.\n",
      "[anndt]: Generated new node with split x_45 > 0.023660315853014723 in train samples separation (2, 15)\n",
      "[anndt]: Generating 43 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 7 new samples.\n",
      "[anndt]: Generated new node with split x_125 > 0.047848055831244414 in train samples separation (2, 13)\n",
      "[anndt]: Generating 45 new samples.\n",
      "[anndt]: Generated new node with split x_227 > 0.13360915209887647 in train samples separation (0, 2)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 11 new samples.\n",
      "[anndt]: Generated new node with split x_203 > 0.11021973202411092 in train samples separation (2, 0)\n",
      "[anndt]: Generating 13 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 5 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_467 > 0.09916744035952071 in train samples separation (42, 283)\n",
      "[anndt]: Generating 8 new samples.\n",
      "[anndt]: Generated new node with split x_374 > 0.029567240996301 in train samples separation (2, 40)\n",
      "[anndt]: Generating 47 new samples.\n",
      "[anndt]: Generated new node with split x_44 > 0.03507143107753198 in train samples separation (0, 2)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 5 new samples.\n",
      "[anndt]: Generated new node with split x_1 > 0.051126782896090656 in train samples separation (0, 2)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 23 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 3 new samples.\n",
      "[anndt]: Generated new node with split x_23 > 0.26909165718349776 in train samples separation (1, 39)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 1 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_35 > 0.01538308327837845 in train samples separation (341, 8377)\n",
      "[anndt]: Generated new node with split x_49 > 0.03138828618036958 in train samples separation (45, 296)\n",
      "[anndt]: Generating 5 new samples.\n",
      "[anndt]: Generated new node with split x_13 > 0.03933960438877961 in train samples separation (5, 40)\n",
      "[anndt]: Generating 44 new samples.\n",
      "[anndt]: Generated new node with split x_86 > 0.18840528879285595 in train samples separation (0, 5)\n",
      "[anndt]: stopping rule - low number of train samples\n",
      "[anndt]: Generating 6 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 6 new samples.\n",
      "[anndt]: Generated new node with split x_389 > 0.0886589892977944 in train samples separation (3, 37)\n",
      "[anndt]: Generating 47 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 3 new samples.\n",
      "[anndt]: Generated new node with split x_0 > 0.024395452762229854 in train samples separation (2, 35)\n",
      "[anndt]: Generating 48 new samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generating 2 new samples.\n",
      "[anndt]: Generated new node with split x_12 > 0.12061375619414348 in train samples separation (1, 34)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_131 > 0.07138082104335486 in train samples separation (516, 7861)\n",
      "[anndt]: Generated new node with split x_131 > 0.18414171863682788 in train samples separation (94, 422)\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_44 > 0.05096901382621156 in train samples separation (79, 343)\n",
      "[anndt]: Generated new node with split x_49 > 0.06458443420971804 in train samples separation (15, 64)\n",
      "[anndt]: Generating 35 new samples.\n",
      "[anndt]: Statistics test passed at confidence level 0.05\n",
      "[anndt]: Generated new node with split x_254 > 0.05724926346704337 in train samples separation (11, 53)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_326 > 0.034571372172120335 in train samples separation (27, 316)\n",
      "[anndt]: Generating 23 new samples.\n",
      "[anndt]: Generated new node with split x_151 > 0.0622402275197308 in train samples separation (12, 15)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_290 > 0.03500765392623481 in train samples separation (888, 6973)\n",
      "[anndt]: Generated new node with split x_49 > 0.033460218763755156 in train samples separation (149, 739)\n",
      "[anndt]: Generated new node with split x_25 > 0.04495345246930296 in train samples separation (30, 119)\n",
      "[anndt]: Generating 20 new samples.\n",
      "[anndt]: Generated new node with split x_16 > 0.04396151590209809 in train samples separation (3, 27)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_103 > 0.052616370233112865 in train samples separation (9, 110)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_483 > 0.05892526062516003 in train samples separation (42, 697)\n",
      "[anndt]: Generating 8 new samples.\n",
      "[anndt]: Generated new node with split x_124 > 0.03818258364256932 in train samples separation (9, 33)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_485 > 0.05171442744174786 in train samples separation (76, 621)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_483 > 0.028733265969517627 in train samples separation (413, 6560)\n",
      "[anndt]: Generated new node with split x_44 > 0.03119428671617195 in train samples separation (102, 311)\n",
      "[anndt]: Generated new node with split x_99 > 0.05732178929409579 in train samples separation (7, 95)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Stopping rule - fraction of the founded node is to low so the leaf is generated.\n",
      "[anndt]: Generated new node with split x_49 > 0.02634784520902215 in train samples separation (1200, 5360)\n",
      "[anndt]: Generated new node with split x_56 > 0.06492605480319284 in train samples separation (41, 1159)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: Generated new node with split x_392 > 0.05638639559513188 in train samples separation (202, 5158)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "[anndt]: stopping rule - max depth exceeded (10)\n",
      "rt.view_graph()\n",
      "Validation 0: fidelity 0.74525, val_fidelity 0.71625, val_accuracy 0.65575\n",
      "Validation 0: fidelity 0.758875, val_fidelity 0.721, val_accuracy 0.6555\n"
     ]
    }
   ],
   "source": [
    "inf = TaT_anndt({\"x\": x_train_final}, {\"y\": y}, {\"x\": x_test_final}, {\"y\": y_test}, tat_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Visualization (.pdf) highlighting extracted rules will be saved in the current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Vocabulary to interpret Decision Tree Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = list(vectorizer.vocabulary_.values())\n",
    "w = list(vectorizer.vocabulary_.keys())\n",
    "voc = {}\n",
    "for i, j in enumerate(v):\n",
    "    voc[j] = w[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('voc Dec8.csv', 'w') as f:\n",
    "    for key in voc.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,voc[key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
